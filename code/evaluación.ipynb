{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def levenshtein_distance(s1, s2):\n",
        "    if len(s1) < len(s2):\n",
        "        return levenshtein_distance(s2, s1)\n",
        "    if len(s2) == 0:\n",
        "        return len(s1)\n",
        "    previous_row = list(range(len(s2) + 1))\n",
        "    for i, c1 in enumerate(s1):\n",
        "        current_row = [i + 1]\n",
        "        for j, c2 in enumerate(s2):\n",
        "            insertions = previous_row[j + 1] + 1\n",
        "            deletions = current_row[j] + 1\n",
        "            substitutions = previous_row[j] + (c1 != c2)\n",
        "            current_row.append(min(insertions, deletions, substitutions))\n",
        "        previous_row = current_row\n",
        "    return previous_row[-1]\n",
        "\n",
        "def levenshtein_distance_words(s1, s2):\n",
        "    words1 = s1.split()\n",
        "    words2 = s2.split()\n",
        "    if len(words1) < len(words2):\n",
        "        return levenshtein_distance_words(s2, s1)\n",
        "    if len(words2) == 0:\n",
        "        return len(words1)\n",
        "    previous_row = list(range(len(words2) + 1))\n",
        "    for i, w1 in enumerate(words1):\n",
        "        current_row = [i + 1]\n",
        "        for j, w2 in enumerate(words2):\n",
        "            insertions = previous_row[j + 1] + 1\n",
        "            deletions = current_row[j] + 1\n",
        "            substitutions = previous_row[j] + (w1 != w2)\n",
        "            current_row.append(min(insertions, deletions, substitutions))\n",
        "        previous_row = current_row\n",
        "    return previous_row[-1]\n",
        "\n",
        "def normalized_levenshtein(s1, s2):\n",
        "    if not s1 and not s2:\n",
        "        return 0.0\n",
        "    if \" \" in s1 or \" \" in s2:\n",
        "        dist = levenshtein_distance_words(s1, s2)\n",
        "        max_len = max(len(s1.split()), len(s2.split()))\n",
        "    else:\n",
        "        dist = levenshtein_distance(s1, s2)\n",
        "        max_len = max(len(s1), len(s2))\n",
        "    return dist / max_len if max_len else 0.0\n",
        "\n",
        "def evaluate_annotations(predictions, references):\n",
        "    TP_exact, TP_partial, FP, FN = 0, 0, 0, 0\n",
        "    partial_distances = []\n",
        "    matched_preds = set()\n",
        "    matched_refs = set()\n",
        "\n",
        "    for i, pred in enumerate(predictions):\n",
        "        matched = False\n",
        "        for j, ref in enumerate(references):\n",
        "            if ref == pred:\n",
        "                TP_exact += 1\n",
        "                matched_preds.add(i)\n",
        "                matched_refs.add(j)\n",
        "                matched = True\n",
        "                break\n",
        "        if not matched:\n",
        "            for j, ref in enumerate(references):\n",
        "                if j in matched_refs:\n",
        "                    continue\n",
        "                if pred in ref or ref in pred:\n",
        "                    TP_partial += 1\n",
        "                    matched_preds.add(i)\n",
        "                    matched_refs.add(j)\n",
        "                    partial_distances.append(normalized_levenshtein(pred, ref))\n",
        "                    break\n",
        "\n",
        "    FP = len(predictions) - len(matched_preds)\n",
        "    FN = len(references) - len(matched_refs)\n",
        "    return {\n",
        "        'TP_exact': TP_exact,\n",
        "        'TP_partial': TP_partial,\n",
        "        'FP': FP,\n",
        "        'FN': FN,\n",
        "        'partial_distances': partial_distances\n",
        "    }\n",
        "\n",
        "# Ruta base\n",
        "base_dir = '/content/drive/My Drive/data_eval'\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "resultados = []\n",
        "global_counts = {'TP_exact': 0, 'TP_partial': 0, 'FP': 0, 'FN': 0, 'partial_distances': []}\n",
        "\n",
        "for i in range(1, 41):\n",
        "    carpeta = f'articulo_{i}'\n",
        "    carpeta_path = os.path.join(base_dir, carpeta)\n",
        "    if not os.path.isdir(carpeta_path):\n",
        "        print(f\"‚ö†Ô∏è Carpeta no encontrada: {carpeta_path}\")\n",
        "        continue\n",
        "\n",
        "    path_expert = os.path.join(carpeta_path, 'terminos_validados_todos.txt')\n",
        "    path_model = os.path.join(carpeta_path, 'terminos_extraidos_con_patrones_ con_ejemplos_mistral.txt')\n",
        "\n",
        "    if not os.path.exists(path_expert) or not os.path.exists(path_model):\n",
        "        print(f\"‚ö†Ô∏è Archivos faltantes en {carpeta}\")\n",
        "        continue\n",
        "\n",
        "    with open(path_expert, 'r', encoding='utf-8') as f:\n",
        "        expert_terms = [line.strip().lower() for line in f if line.strip()]\n",
        "\n",
        "    with open(path_model, 'r', encoding='utf-8') as f:\n",
        "        candidate_terms = [line.strip().lower() for line in f if line.strip()]\n",
        "\n",
        "    r = evaluate_annotations(candidate_terms, expert_terms)\n",
        "    TP_total = r['TP_exact'] + r['TP_partial']\n",
        "    precision = TP_total / (TP_total + r['FP']) if TP_total + r['FP'] > 0 else 0.0\n",
        "    recall = TP_total / (TP_total + r['FN']) if TP_total + r['FN'] > 0 else 0.0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0.0\n",
        "    f2 = 5 * precision * recall / (4 * precision + recall) if precision + recall > 0 else 0.0\n",
        "    avg_lev = sum(r['partial_distances']) / len(r['partial_distances']) if r['partial_distances'] else None\n",
        "\n",
        "    resultados.append({\n",
        "        'Art√≠culo': carpeta,\n",
        "        'TP_exact': r['TP_exact'],\n",
        "        'TP_partial': r['TP_partial'],\n",
        "        'FP': r['FP'],\n",
        "        'FN': r['FN'],\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1': f1,\n",
        "        'F2': f2,\n",
        "        'Avg_Norm_Levenshtein': avg_lev\n",
        "    })\n",
        "\n",
        "    for k in ['TP_exact', 'TP_partial', 'FP', 'FN']:\n",
        "        global_counts[k] += r[k]\n",
        "    global_counts['partial_distances'].extend(r['partial_distances'])\n",
        "\n",
        "TP_total = global_counts['TP_exact'] + global_counts['TP_partial']\n",
        "FP = global_counts['FP']\n",
        "FN = global_counts['FN']\n",
        "precision = TP_total / (TP_total + FP) if TP_total + FP > 0 else 0.0\n",
        "recall = TP_total / (TP_total + FN) if TP_total + FN > 0 else 0.0\n",
        "f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0.0\n",
        "f2 = 5 * precision * recall / (4 * precision + recall) if precision + recall > 0 else 0.0\n",
        "avg_lev = sum(global_counts['partial_distances']) / len(global_counts['partial_distances']) if global_counts['partial_distances'] else None\n",
        "\n",
        "ruta_resultados_por_articulo = os.path.join(base_dir, 'evaluacion_anotaciones_por_articulo_mistral_one.csv')\n",
        "ruta_resultados_global = os.path.join(base_dir, 'evaluacion_anotaciones_global_mistral_one.csv')\n",
        "\n",
        "try:\n",
        "    df_resultados = pd.DataFrame(resultados)\n",
        "    df_resultados.to_csv(ruta_resultados_por_articulo, index=False, encoding='utf-8')\n",
        "    print(f\"üìÑ Resultados por art√≠culo guardados en: {ruta_resultados_por_articulo}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error al guardar resultados por art√≠culo: {e}\")\n",
        "\n",
        "try:\n",
        "    df_global = pd.DataFrame([{\n",
        "        'TP_exact': global_counts['TP_exact'],\n",
        "        'TP_partial': global_counts['TP_partial'],\n",
        "        'FP': FP,\n",
        "        'FN': FN,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1': f1,\n",
        "        'F2': f2,\n",
        "        'Avg_Norm_Levenshtein': avg_lev\n",
        "    }])\n",
        "    df_global.to_csv(ruta_resultados_global, index=False, encoding='utf-8')\n",
        "    print(f\"üìÑ Resultados globales guardados en: {ruta_resultados_global}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error al guardar resultados globales: {e}\")\n",
        "\n",
        "print(\"‚úÖ Evaluaci√≥n completada.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFoKtJUPmT24",
        "outputId": "51b821d8-cae2-4ec6-8f94-81ae6fd2f612"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÑ Resultados por art√≠culo guardados en: /content/drive/My Drive/data_eval/evaluacion_anotaciones_por_articulo_mistral_one.csv\n",
            "üìÑ Resultados globales guardados en: /content/drive/My Drive/data_eval/evaluacion_anotaciones_global_mistral_one.csv\n",
            "‚úÖ Evaluaci√≥n completada.\n"
          ]
        }
      ]
    }
  ]
}