{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fed061e5-9b3d-4c97-a878-426fec9b65ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting git+https://github.com/boudinfl/pke.git\n",
      "  Cloning https://github.com/boudinfl/pke.git to /tmp/pip-req-build-g2axy5op\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/boudinfl/pke.git /tmp/pip-req-build-g2axy5op\n",
      "  Resolved https://github.com/boudinfl/pke.git to commit 69871ffdb720b83df23684fea53ec8776fd87e63\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from pke==2.0.0) (2.8.6)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pke==2.0.0) (1.22.4)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from pke==2.0.0) (1.9.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from pke==2.0.0) (1.1.2)\n",
      "Collecting unidecode\n",
      "  Downloading Unidecode-1.4.0-py3-none-any.whl (235 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting future\n",
      "  Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m491.3/491.3 kB\u001b[0m \u001b[31m112.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from pke==2.0.0) (1.1.0)\n",
      "Collecting spacy>=3.2.3\n",
      "  Downloading spacy-3.8.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.5 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.5/31.5 MB\u001b[0m \u001b[31m114.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting thinc<8.4.0,>=8.3.4\n",
      "  Downloading thinc-8.3.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.13-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (117 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m117.1/117.1 kB\u001b[0m \u001b[31m218.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0\n",
      "  Downloading typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m224.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m183.0/183.0 kB\u001b[0m \u001b[31m427.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy>=3.2.3->pke==2.0.0) (3.1.2)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m129.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy>=3.2.3->pke==2.0.0) (4.64.1)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy>=3.2.3->pke==2.0.0) (2.28.1)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (204 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m204.8/204.8 kB\u001b[0m \u001b[31m126.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy>=3.2.3->pke==2.0.0) (65.3.0)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.10-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (795 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m795.1/795.1 kB\u001b[0m \u001b[31m102.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting weasel<0.5.0,>=0.1.0\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.3/50.3 kB\u001b[0m \u001b[31m319.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy>=3.2.3->pke==2.0.0) (23.2)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk->pke==2.0.0) (8.1.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk->pke==2.0.0) (2023.12.25)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->pke==2.0.0) (3.1.0)\n",
      "Collecting language-data>=1.2\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting typing-extensions>=4.12.2\n",
      "  Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m319.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting annotated-types>=0.6.0\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting pydantic-core==2.33.2\n",
      "  Downloading pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-inspection>=0.4.0\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2022.6.15.1)\n",
      "Collecting thinc<8.4.0,>=8.3.4\n",
      "  Downloading thinc-8.3.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Collecting blis<1.3.0,>=1.2.0\n",
      "  Downloading blis-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m115.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting rich>=10.11.0\n",
      "  Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m243.2/243.2 kB\u001b[0m \u001b[31m178.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting shellingham>=1.3.0\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1\n",
      "  Downloading smart_open-7.3.0.post1-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m350.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cloudpathlib<1.0.0,>=0.7.0\n",
      "  Downloading cloudpathlib-0.21.1-py3-none-any.whl (52 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m340.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy>=3.2.3->pke==2.0.0) (2.1.1)\n",
      "Collecting marisa-trie>=1.1.0\n",
      "  Downloading marisa_trie-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m117.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (2.13.0)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m377.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=3.2.3->pke==2.0.0) (1.14.1)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Building wheels for collected packages: pke\n",
      "  Building wheel for pke (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pke: filename=pke-2.0.0-py3-none-any.whl size=6160630 sha256=81821a248aa244d4ec629369a648a1d3231606cbcef6e1764cdecf544eef77d5\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-cznzcokp/wheels/8c/07/29/6b35bed2aa36e33d77ff3677eb716965ece4d2e56639ad0aab\n",
      "Successfully built pke\n",
      "Installing collected packages: cymem, wasabi, unidecode, typing-extensions, spacy-loggers, spacy-legacy, smart-open, shellingham, nltk, murmurhash, mdurl, marisa-trie, future, catalogue, blis, annotated-types, typing-inspection, srsly, pydantic-core, preshed, markdown-it-py, language-data, cloudpathlib, rich, pydantic, langcodes, typer, confection, weasel, thinc, spacy, pke\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.3.0\n",
      "    Uninstalling typing_extensions-4.3.0:\n",
      "      Successfully uninstalled typing_extensions-4.3.0\n",
      "Successfully installed annotated-types-0.7.0 blis-1.2.1 catalogue-2.0.10 cloudpathlib-0.21.1 confection-0.1.5 cymem-2.0.11 future-1.0.0 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.13 nltk-3.9.1 pke-2.0.0 preshed-3.0.10 pydantic-2.11.7 pydantic-core-2.33.2 rich-14.0.0 shellingham-1.5.4 smart-open-7.3.0.post1 spacy-3.8.7 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.4 typer-0.16.0 typing-extensions-4.14.1 typing-inspection-0.4.1 unidecode-1.4.0 wasabi-1.1.3 weasel-0.4.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/boudinfl/pke.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b940f92-5126-4819-8bbd-28fd79d6d749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting es-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: es-core-news-sm\n",
      "Successfully installed es-core-news-sm-3.8.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!spacy download es_core_news_sm\n",
    "import spacy\n",
    "from spacy.lang.es.examples import sentences\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35c3d545-61cf-459e-894c-5bf5752e1319",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ee8e3af-e570-4ec6-8398-07276b48bcd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Extrayendo tÃ©rminos con YAKE...\n",
      "âœ… ArtÃ­culo 1: 247 tÃ©rminos extraÃ­dos\n",
      "âœ… ArtÃ­culo 2: 112 tÃ©rminos extraÃ­dos\n",
      "âœ… ArtÃ­culo 3: 121 tÃ©rminos extraÃ­dos\n",
      "âœ… ArtÃ­culo 4: 157 tÃ©rminos extraÃ­dos\n",
      "âœ… ArtÃ­culo 5: 50 tÃ©rminos extraÃ­dos\n",
      "âœ… ArtÃ­culo 6: 68 tÃ©rminos extraÃ­dos\n",
      "âœ… ArtÃ­culo 7: 57 tÃ©rminos extraÃ­dos\n",
      "âœ… ArtÃ­culo 8: 258 tÃ©rminos extraÃ­dos\n",
      "âœ… ArtÃ­culo 9: 80 tÃ©rminos extraÃ­dos\n",
      "âœ… ArtÃ­culo 10: 42 tÃ©rminos extraÃ­dos\n",
      "âœ… ArtÃ­culo 11: 721 tÃ©rminos extraÃ­dos\n",
      "âœ… ArtÃ­culo 12: 564 tÃ©rminos extraÃ­dos\n",
      "âœ… ArtÃ­culo 13: 17 tÃ©rminos extraÃ­dos\n",
      "âœ… ArtÃ­culo 14: 128 tÃ©rminos extraÃ­dos\n",
      "âœ… ArtÃ­culo 15: 494 tÃ©rminos extraÃ­dos\n",
      "âœ… ArtÃ­culo 16: 327 tÃ©rminos extraÃ­dos\n",
      "âœ… ArtÃ­culo 17: 242 tÃ©rminos extraÃ­dos\n",
      "âœ… ArtÃ­culo 18: 32 tÃ©rminos extraÃ­dos\n",
      "âœ… ArtÃ­culo 19: 164 tÃ©rminos extraÃ­dos\n",
      "âœ… ArtÃ­culo 20: 129 tÃ©rminos extraÃ­dos\n",
      "âœ… ArtÃ­culo 21: 104 tÃ©rminos extraÃ­dos\n",
      "âœ… ArtÃ­culo 22: 91 tÃ©rminos extraÃ­dos\n",
      "âœ… ArtÃ­culo 23: 143 tÃ©rminos extraÃ­dos\n",
      "âœ… ArtÃ­culo 24: 58 tÃ©rminos extraÃ­dos\n",
      "âœ… ArtÃ­culo 25: 39 tÃ©rminos extraÃ­dos\n",
      "âœ… ArtÃ­culo 26: 159 tÃ©rminos extraÃ­dos\n",
      "âœ… ArtÃ­culo 27: 111 tÃ©rminos extraÃ­dos\n",
      "âœ… ArtÃ­culo 28: 114 tÃ©rminos extraÃ­dos\n",
      "âœ… ArtÃ­culo 29: 142 tÃ©rminos extraÃ­dos\n",
      "âœ… ArtÃ­culo 30: 26 tÃ©rminos extraÃ­dos\n",
      "âœ… ArtÃ­culo 31: 29 tÃ©rminos extraÃ­dos\n",
      "âœ… ArtÃ­culo 32: 112 tÃ©rminos extraÃ­dos\n",
      "âœ… ArtÃ­culo 33: 564 tÃ©rminos extraÃ­dos\n",
      "âœ… ArtÃ­culo 34: 384 tÃ©rminos extraÃ­dos\n",
      "âœ… ArtÃ­culo 35: 142 tÃ©rminos extraÃ­dos\n",
      "âœ… ArtÃ­culo 36: 236 tÃ©rminos extraÃ­dos\n",
      "âœ… ArtÃ­culo 37: 709 tÃ©rminos extraÃ­dos\n",
      "âœ… ArtÃ­culo 38: 122 tÃ©rminos extraÃ­dos\n",
      "âœ… ArtÃ­culo 39: 151 tÃ©rminos extraÃ­dos\n",
      "âœ… ArtÃ­culo 40: 485 tÃ©rminos extraÃ­dos\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import pke\n",
    "from pke.lang import stopwords\n",
    "\n",
    "\n",
    "# --- Tokenizer fix para palabras con guiones ---\n",
    "from spacy.tokenizer import _get_regex_pattern\n",
    "re_token_match = _get_regex_pattern(nlp.Defaults.token_match)\n",
    "re_token_match = f\"({re_token_match}|\\w+-\\w+)\"\n",
    "nlp.tokenizer.token_match = re.compile(re_token_match).match\n",
    "\n",
    "# Ruta base de los artÃ­culos\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "# Diccionario para almacenar resultados de YAKE\n",
    "resultados_yake = {}\n",
    "\n",
    "# --- ExtracciÃ³n de tÃ©rminos con YAKE ---\n",
    "print(\"ğŸ” Extrayendo tÃ©rminos con YAKE...\")\n",
    "for i in range(1, 41):\n",
    "    carpeta = os.path.join(base_dir, f\"articulo_{i}\")\n",
    "    archivo_txt = os.path.join(carpeta, f\"articulo_{i}.txt\")\n",
    "\n",
    "    if not os.path.exists(archivo_txt):\n",
    "        print(f\"âš ï¸ Archivo no encontrado: {archivo_txt}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        with open(archivo_txt, \"r\", encoding=\"utf-8\") as f:\n",
    "            texto = f.read().strip()\n",
    "\n",
    "        if not texto:\n",
    "            print(f\"âš ï¸ ArtÃ­culo {i} vacÃ­o.\")\n",
    "            continue\n",
    "\n",
    "        extractor = pke.unsupervised.YAKE()\n",
    "        extractor.load_document(input=texto,\n",
    "                                language='es',\n",
    "                                stoplist=stopwords.get('es'),\n",
    "                                normalization=None)\n",
    "\n",
    "        extractor.candidate_selection(n=3)\n",
    "        extractor.candidate_weighting(window=2, use_stems=False)\n",
    "\n",
    "        # Obtener todos los candidatos ordenados por puntuaciÃ³n\n",
    "        keyphrases = sorted(extractor.weights.items(), key=lambda x: x[1])\n",
    "        resultados_yake[f\"articulo_{i}\"] = keyphrases\n",
    "\n",
    "        # Guardar en archivo para evaluaciÃ³n\n",
    "        path_out = os.path.join(carpeta, 'terminos_extraidos_yake.txt')\n",
    "        with open(path_out, 'w', encoding='utf-8') as f_out:\n",
    "            for termino, _ in keyphrases:\n",
    "                f_out.write(termino.strip().lower() + '\\n')\n",
    "\n",
    "        print(f\"âœ… ArtÃ­culo {i}: {len(keyphrases)} tÃ©rminos extraÃ­dos\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error procesando artÃ­culo {i}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f4ae17a-6359-4ce7-b37c-c9dd2070e32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Evaluando tÃ©rminos extraÃ­dos con YAKE...\n",
      "ğŸ“„ Resultados por artÃ­culo guardados en: data/evaluacion_anotaciones_por_articulo_yake.csv\n",
      "ğŸ“„ Resultados globales guardados en: data/evaluacion_anotaciones_global_yake.csv\n",
      "\n",
      "âœ… EvaluaciÃ³n completada.\n"
     ]
    }
   ],
   "source": [
    "# --- Funciones para evaluaciÃ³n ---\n",
    "def levenshtein_distance(s1, s2):\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein_distance(s2, s1)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "    previous_row = list(range(len(s2) + 1))\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1\n",
    "            deletions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    return previous_row[-1]\n",
    "\n",
    "def levenshtein_distance_words(s1, s2):\n",
    "    words1 = s1.split()\n",
    "    words2 = s2.split()\n",
    "    if len(words1) < len(words2):\n",
    "        return levenshtein_distance_words(s2, s1)\n",
    "    if len(words2) == 0:\n",
    "        return len(words1)\n",
    "    previous_row = list(range(len(words2) + 1))\n",
    "    for i, w1 in enumerate(words1):\n",
    "        current_row = [i + 1]\n",
    "        for j, w2 in enumerate(words2):\n",
    "            insertions = previous_row[j + 1] + 1\n",
    "            deletions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (w1 != w2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    return previous_row[-1]\n",
    "\n",
    "def normalized_levenshtein(s1, s2):\n",
    "    if not s1 and not s2:\n",
    "        return 0.0\n",
    "    if \" \" in s1 or \" \" in s2:\n",
    "        dist = levenshtein_distance_words(s1, s2)\n",
    "        max_len = max(len(s1.split()), len(s2.split()))\n",
    "    else:\n",
    "        dist = levenshtein_distance(s1, s2)\n",
    "        max_len = max(len(s1), len(s2))\n",
    "    return dist / max_len if max_len else 0.0\n",
    "\n",
    "def evaluate_annotations(predictions, references):\n",
    "    TP_exact, TP_partial, FP, FN = 0, 0, 0, 0\n",
    "    partial_distances = []\n",
    "    matched_preds = set()\n",
    "    matched_refs = set()\n",
    "\n",
    "    for i, pred in enumerate(predictions):\n",
    "        matched = False\n",
    "        for j, ref in enumerate(references):\n",
    "            if ref == pred:\n",
    "                TP_exact += 1\n",
    "                matched_preds.add(i)\n",
    "                matched_refs.add(j)\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            for j, ref in enumerate(references):\n",
    "                if j in matched_refs:\n",
    "                    continue\n",
    "                if pred in ref or ref in pred:\n",
    "                    TP_partial += 1\n",
    "                    matched_preds.add(i)\n",
    "                    matched_refs.add(j)\n",
    "                    partial_distances.append(normalized_levenshtein(pred, ref))\n",
    "                    break\n",
    "\n",
    "    FP = len(predictions) - len(matched_preds)\n",
    "    FN = len(references) - len(matched_refs)\n",
    "    return {\n",
    "        'TP_exact': TP_exact,\n",
    "        'TP_partial': TP_partial,\n",
    "        'FP': FP,\n",
    "        'FN': FN,\n",
    "        'partial_distances': partial_distances\n",
    "    }\n",
    "\n",
    "# --- EvaluaciÃ³n ---\n",
    "print(\"\\nğŸ“Š Evaluando tÃ©rminos extraÃ­dos con YAKE...\")\n",
    "resultados = []\n",
    "global_counts = {'TP_exact': 0, 'TP_partial': 0, 'FP': 0, 'FN': 0, 'partial_distances': []}\n",
    "\n",
    "for i in range(1, 41):\n",
    "    carpeta = f'articulo_{i}'\n",
    "    carpeta_path = os.path.join(base_dir, carpeta)\n",
    "    path_expert = os.path.join(carpeta_path, 'terminos_validados_todos.txt')\n",
    "    path_model = os.path.join(carpeta_path, 'terminos_extraidos_yake.txt')\n",
    "\n",
    "    if not os.path.exists(path_expert) or not os.path.exists(path_model):\n",
    "        print(f\"âš ï¸ Archivos faltantes en {carpeta}\")\n",
    "        continue\n",
    "\n",
    "    with open(path_expert, 'r', encoding='utf-8') as f:\n",
    "        expert_terms = [line.strip().lower() for line in f if line.strip()]\n",
    "\n",
    "    with open(path_model, 'r', encoding='utf-8') as f:\n",
    "        candidate_terms = [line.strip().lower() for line in f if line.strip()]\n",
    "\n",
    "    r = evaluate_annotations(candidate_terms, expert_terms)\n",
    "    TP_total = r['TP_exact'] + r['TP_partial']\n",
    "    precision = TP_total / (TP_total + r['FP']) if TP_total + r['FP'] > 0 else 0.0\n",
    "    recall = TP_total / (TP_total + r['FN']) if TP_total + r['FN'] > 0 else 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0.0\n",
    "    f2 = 5 * precision * recall / (4 * precision + recall) if precision + recall > 0 else 0.0\n",
    "    avg_lev = sum(r['partial_distances']) / len(r['partial_distances']) if r['partial_distances'] else None\n",
    "\n",
    "    resultados.append({\n",
    "        'ArtÃ­culo': carpeta,\n",
    "        'TP_exact': r['TP_exact'],\n",
    "        'TP_partial': r['TP_partial'],\n",
    "        'FP': r['FP'],\n",
    "        'FN': r['FN'],\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1': f1,\n",
    "        'F2': f2,\n",
    "        'Avg_Norm_Levenshtein': avg_lev\n",
    "    })\n",
    "\n",
    "    for k in ['TP_exact', 'TP_partial', 'FP', 'FN']:\n",
    "        global_counts[k] += r[k]\n",
    "    global_counts['partial_distances'].extend(r['partial_distances'])\n",
    "\n",
    "# Resultados globales\n",
    "TP_total = global_counts['TP_exact'] + global_counts['TP_partial']\n",
    "FP = global_counts['FP']\n",
    "FN = global_counts['FN']\n",
    "precision = TP_total / (TP_total + FP) if TP_total + FP > 0 else 0.0\n",
    "recall = TP_total / (TP_total + FN) if TP_total + FN > 0 else 0.0\n",
    "f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0.0\n",
    "f2 = 5 * precision * recall / (4 * precision + recall) if precision + recall > 0 else 0.0\n",
    "avg_lev = sum(global_counts['partial_distances']) / len(global_counts['partial_distances']) if global_counts['partial_distances'] else None\n",
    "\n",
    "# Exportar resultados\n",
    "ruta_resultados_por_articulo = os.path.join(base_dir, 'evaluacion_anotaciones_por_articulo_yake.csv')\n",
    "ruta_resultados_global = os.path.join(base_dir, 'evaluacion_anotaciones_global_yake.csv')\n",
    "\n",
    "try:\n",
    "    df_resultados = pd.DataFrame(resultados)\n",
    "    df_resultados.to_csv(ruta_resultados_por_articulo, index=False, encoding='utf-8')\n",
    "    print(f\"ğŸ“„ Resultados por artÃ­culo guardados en: {ruta_resultados_por_articulo}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error al guardar resultados por artÃ­culo: {e}\")\n",
    "\n",
    "try:\n",
    "    df_global = pd.DataFrame([{\n",
    "        'TP_exact': global_counts['TP_exact'],\n",
    "        'TP_partial': global_counts['TP_partial'],\n",
    "        'FP': FP,\n",
    "        'FN': FN,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1': f1,\n",
    "        'F2': f2,\n",
    "        'Avg_Norm_Levenshtein': avg_lev\n",
    "    }])\n",
    "    df_global.to_csv(ruta_resultados_global, index=False, encoding='utf-8')\n",
    "    print(f\"ğŸ“„ Resultados globales guardados en: {ruta_resultados_global}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error al guardar resultados globales: {e}\")\n",
    "\n",
    "print(\"\\nâœ… EvaluaciÃ³n completada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63744c25-1b68-4a09-ab1e-776ebed6aee0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
