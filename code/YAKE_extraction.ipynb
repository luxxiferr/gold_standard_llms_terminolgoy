{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fed061e5-9b3d-4c97-a878-426fec9b65ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting git+https://github.com/boudinfl/pke.git\n",
      "  Cloning https://github.com/boudinfl/pke.git to /tmp/pip-req-build-g2axy5op\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/boudinfl/pke.git /tmp/pip-req-build-g2axy5op\n",
      "  Resolved https://github.com/boudinfl/pke.git to commit 69871ffdb720b83df23684fea53ec8776fd87e63\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from pke==2.0.0) (2.8.6)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pke==2.0.0) (1.22.4)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from pke==2.0.0) (1.9.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from pke==2.0.0) (1.1.2)\n",
      "Collecting unidecode\n",
      "  Downloading Unidecode-1.4.0-py3-none-any.whl (235 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting future\n",
      "  Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.3/491.3 kB\u001b[0m \u001b[31m112.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from pke==2.0.0) (1.1.0)\n",
      "Collecting spacy>=3.2.3\n",
      "  Downloading spacy-3.8.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.5/31.5 MB\u001b[0m \u001b[31m114.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting thinc<8.4.0,>=8.3.4\n",
      "  Downloading thinc-8.3.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.13-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (117 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.1/117.1 kB\u001b[0m \u001b[31m218.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0\n",
      "  Downloading typer-0.16.0-py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m224.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.0/183.0 kB\u001b[0m \u001b[31m427.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy>=3.2.3->pke==2.0.0) (3.1.2)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m129.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy>=3.2.3->pke==2.0.0) (4.64.1)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy>=3.2.3->pke==2.0.0) (2.28.1)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (204 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.8/204.8 kB\u001b[0m \u001b[31m126.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy>=3.2.3->pke==2.0.0) (65.3.0)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.10-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (795 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m795.1/795.1 kB\u001b[0m \u001b[31m102.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting weasel<0.5.0,>=0.1.0\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/50.3 kB\u001b[0m \u001b[31m319.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy>=3.2.3->pke==2.0.0) (23.2)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk->pke==2.0.0) (8.1.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk->pke==2.0.0) (2023.12.25)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->pke==2.0.0) (3.1.0)\n",
      "Collecting language-data>=1.2\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting typing-extensions>=4.12.2\n",
      "  Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m319.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting annotated-types>=0.6.0\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting pydantic-core==2.33.2\n",
      "  Downloading pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-inspection>=0.4.0\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2022.6.15.1)\n",
      "Collecting thinc<8.4.0,>=8.3.4\n",
      "  Downloading thinc-8.3.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Collecting blis<1.3.0,>=1.2.0\n",
      "  Downloading blis-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m115.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting rich>=10.11.0\n",
      "  Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.2/243.2 kB\u001b[0m \u001b[31m178.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting shellingham>=1.3.0\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1\n",
      "  Downloading smart_open-7.3.0.post1-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m350.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cloudpathlib<1.0.0,>=0.7.0\n",
      "  Downloading cloudpathlib-0.21.1-py3-none-any.whl (52 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m340.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy>=3.2.3->pke==2.0.0) (2.1.1)\n",
      "Collecting marisa-trie>=1.1.0\n",
      "  Downloading marisa_trie-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m117.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (2.13.0)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m377.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wrapt in /opt/conda/lib/python3.10/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=3.2.3->pke==2.0.0) (1.14.1)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Building wheels for collected packages: pke\n",
      "  Building wheel for pke (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pke: filename=pke-2.0.0-py3-none-any.whl size=6160630 sha256=81821a248aa244d4ec629369a648a1d3231606cbcef6e1764cdecf544eef77d5\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-cznzcokp/wheels/8c/07/29/6b35bed2aa36e33d77ff3677eb716965ece4d2e56639ad0aab\n",
      "Successfully built pke\n",
      "Installing collected packages: cymem, wasabi, unidecode, typing-extensions, spacy-loggers, spacy-legacy, smart-open, shellingham, nltk, murmurhash, mdurl, marisa-trie, future, catalogue, blis, annotated-types, typing-inspection, srsly, pydantic-core, preshed, markdown-it-py, language-data, cloudpathlib, rich, pydantic, langcodes, typer, confection, weasel, thinc, spacy, pke\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.3.0\n",
      "    Uninstalling typing_extensions-4.3.0:\n",
      "      Successfully uninstalled typing_extensions-4.3.0\n",
      "Successfully installed annotated-types-0.7.0 blis-1.2.1 catalogue-2.0.10 cloudpathlib-0.21.1 confection-0.1.5 cymem-2.0.11 future-1.0.0 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.13 nltk-3.9.1 pke-2.0.0 preshed-3.0.10 pydantic-2.11.7 pydantic-core-2.33.2 rich-14.0.0 shellingham-1.5.4 smart-open-7.3.0.post1 spacy-3.8.7 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.4 typer-0.16.0 typing-extensions-4.14.1 typing-inspection-0.4.1 unidecode-1.4.0 wasabi-1.1.3 weasel-0.4.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/boudinfl/pke.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b940f92-5126-4819-8bbd-28fd79d6d749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting es-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: es-core-news-sm\n",
      "Successfully installed es-core-news-sm-3.8.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!spacy download es_core_news_sm\n",
    "import spacy\n",
    "from spacy.lang.es.examples import sentences\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35c3d545-61cf-459e-894c-5bf5752e1319",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ee8e3af-e570-4ec6-8398-07276b48bcd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Extrayendo términos con YAKE...\n",
      "✅ Artículo 1: 247 términos extraídos\n",
      "✅ Artículo 2: 112 términos extraídos\n",
      "✅ Artículo 3: 121 términos extraídos\n",
      "✅ Artículo 4: 157 términos extraídos\n",
      "✅ Artículo 5: 50 términos extraídos\n",
      "✅ Artículo 6: 68 términos extraídos\n",
      "✅ Artículo 7: 57 términos extraídos\n",
      "✅ Artículo 8: 258 términos extraídos\n",
      "✅ Artículo 9: 80 términos extraídos\n",
      "✅ Artículo 10: 42 términos extraídos\n",
      "✅ Artículo 11: 721 términos extraídos\n",
      "✅ Artículo 12: 564 términos extraídos\n",
      "✅ Artículo 13: 17 términos extraídos\n",
      "✅ Artículo 14: 128 términos extraídos\n",
      "✅ Artículo 15: 494 términos extraídos\n",
      "✅ Artículo 16: 327 términos extraídos\n",
      "✅ Artículo 17: 242 términos extraídos\n",
      "✅ Artículo 18: 32 términos extraídos\n",
      "✅ Artículo 19: 164 términos extraídos\n",
      "✅ Artículo 20: 129 términos extraídos\n",
      "✅ Artículo 21: 104 términos extraídos\n",
      "✅ Artículo 22: 91 términos extraídos\n",
      "✅ Artículo 23: 143 términos extraídos\n",
      "✅ Artículo 24: 58 términos extraídos\n",
      "✅ Artículo 25: 39 términos extraídos\n",
      "✅ Artículo 26: 159 términos extraídos\n",
      "✅ Artículo 27: 111 términos extraídos\n",
      "✅ Artículo 28: 114 términos extraídos\n",
      "✅ Artículo 29: 142 términos extraídos\n",
      "✅ Artículo 30: 26 términos extraídos\n",
      "✅ Artículo 31: 29 términos extraídos\n",
      "✅ Artículo 32: 112 términos extraídos\n",
      "✅ Artículo 33: 564 términos extraídos\n",
      "✅ Artículo 34: 384 términos extraídos\n",
      "✅ Artículo 35: 142 términos extraídos\n",
      "✅ Artículo 36: 236 términos extraídos\n",
      "✅ Artículo 37: 709 términos extraídos\n",
      "✅ Artículo 38: 122 términos extraídos\n",
      "✅ Artículo 39: 151 términos extraídos\n",
      "✅ Artículo 40: 485 términos extraídos\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import pke\n",
    "from pke.lang import stopwords\n",
    "\n",
    "\n",
    "# --- Tokenizer fix para palabras con guiones ---\n",
    "from spacy.tokenizer import _get_regex_pattern\n",
    "re_token_match = _get_regex_pattern(nlp.Defaults.token_match)\n",
    "re_token_match = f\"({re_token_match}|\\w+-\\w+)\"\n",
    "nlp.tokenizer.token_match = re.compile(re_token_match).match\n",
    "\n",
    "# Ruta base de los artículos\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "# Diccionario para almacenar resultados de YAKE\n",
    "resultados_yake = {}\n",
    "\n",
    "# --- Extracción de términos con YAKE ---\n",
    "print(\"🔍 Extrayendo términos con YAKE...\")\n",
    "for i in range(1, 41):\n",
    "    carpeta = os.path.join(base_dir, f\"articulo_{i}\")\n",
    "    archivo_txt = os.path.join(carpeta, f\"articulo_{i}.txt\")\n",
    "\n",
    "    if not os.path.exists(archivo_txt):\n",
    "        print(f\"⚠️ Archivo no encontrado: {archivo_txt}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        with open(archivo_txt, \"r\", encoding=\"utf-8\") as f:\n",
    "            texto = f.read().strip()\n",
    "\n",
    "        if not texto:\n",
    "            print(f\"⚠️ Artículo {i} vacío.\")\n",
    "            continue\n",
    "\n",
    "        extractor = pke.unsupervised.YAKE()\n",
    "        extractor.load_document(input=texto,\n",
    "                                language='es',\n",
    "                                stoplist=stopwords.get('es'),\n",
    "                                normalization=None)\n",
    "\n",
    "        extractor.candidate_selection(n=3)\n",
    "        extractor.candidate_weighting(window=2, use_stems=False)\n",
    "\n",
    "        # Obtener todos los candidatos ordenados por puntuación\n",
    "        keyphrases = sorted(extractor.weights.items(), key=lambda x: x[1])\n",
    "        resultados_yake[f\"articulo_{i}\"] = keyphrases\n",
    "\n",
    "        # Guardar en archivo para evaluación\n",
    "        path_out = os.path.join(carpeta, 'terminos_extraidos_yake.txt')\n",
    "        with open(path_out, 'w', encoding='utf-8') as f_out:\n",
    "            for termino, _ in keyphrases:\n",
    "                f_out.write(termino.strip().lower() + '\\n')\n",
    "\n",
    "        print(f\"✅ Artículo {i}: {len(keyphrases)} términos extraídos\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error procesando artículo {i}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f4ae17a-6359-4ce7-b37c-c9dd2070e32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Evaluando términos extraídos con YAKE...\n",
      "📄 Resultados por artículo guardados en: data/evaluacion_anotaciones_por_articulo_yake.csv\n",
      "📄 Resultados globales guardados en: data/evaluacion_anotaciones_global_yake.csv\n",
      "\n",
      "✅ Evaluación completada.\n"
     ]
    }
   ],
   "source": [
    "# --- Funciones para evaluación ---\n",
    "def levenshtein_distance(s1, s2):\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein_distance(s2, s1)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "    previous_row = list(range(len(s2) + 1))\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1\n",
    "            deletions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    return previous_row[-1]\n",
    "\n",
    "def levenshtein_distance_words(s1, s2):\n",
    "    words1 = s1.split()\n",
    "    words2 = s2.split()\n",
    "    if len(words1) < len(words2):\n",
    "        return levenshtein_distance_words(s2, s1)\n",
    "    if len(words2) == 0:\n",
    "        return len(words1)\n",
    "    previous_row = list(range(len(words2) + 1))\n",
    "    for i, w1 in enumerate(words1):\n",
    "        current_row = [i + 1]\n",
    "        for j, w2 in enumerate(words2):\n",
    "            insertions = previous_row[j + 1] + 1\n",
    "            deletions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (w1 != w2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        previous_row = current_row\n",
    "    return previous_row[-1]\n",
    "\n",
    "def normalized_levenshtein(s1, s2):\n",
    "    if not s1 and not s2:\n",
    "        return 0.0\n",
    "    if \" \" in s1 or \" \" in s2:\n",
    "        dist = levenshtein_distance_words(s1, s2)\n",
    "        max_len = max(len(s1.split()), len(s2.split()))\n",
    "    else:\n",
    "        dist = levenshtein_distance(s1, s2)\n",
    "        max_len = max(len(s1), len(s2))\n",
    "    return dist / max_len if max_len else 0.0\n",
    "\n",
    "def evaluate_annotations(predictions, references):\n",
    "    TP_exact, TP_partial, FP, FN = 0, 0, 0, 0\n",
    "    partial_distances = []\n",
    "    matched_preds = set()\n",
    "    matched_refs = set()\n",
    "\n",
    "    for i, pred in enumerate(predictions):\n",
    "        matched = False\n",
    "        for j, ref in enumerate(references):\n",
    "            if ref == pred:\n",
    "                TP_exact += 1\n",
    "                matched_preds.add(i)\n",
    "                matched_refs.add(j)\n",
    "                matched = True\n",
    "                break\n",
    "        if not matched:\n",
    "            for j, ref in enumerate(references):\n",
    "                if j in matched_refs:\n",
    "                    continue\n",
    "                if pred in ref or ref in pred:\n",
    "                    TP_partial += 1\n",
    "                    matched_preds.add(i)\n",
    "                    matched_refs.add(j)\n",
    "                    partial_distances.append(normalized_levenshtein(pred, ref))\n",
    "                    break\n",
    "\n",
    "    FP = len(predictions) - len(matched_preds)\n",
    "    FN = len(references) - len(matched_refs)\n",
    "    return {\n",
    "        'TP_exact': TP_exact,\n",
    "        'TP_partial': TP_partial,\n",
    "        'FP': FP,\n",
    "        'FN': FN,\n",
    "        'partial_distances': partial_distances\n",
    "    }\n",
    "\n",
    "# --- Evaluación ---\n",
    "print(\"\\n📊 Evaluando términos extraídos con YAKE...\")\n",
    "resultados = []\n",
    "global_counts = {'TP_exact': 0, 'TP_partial': 0, 'FP': 0, 'FN': 0, 'partial_distances': []}\n",
    "\n",
    "for i in range(1, 41):\n",
    "    carpeta = f'articulo_{i}'\n",
    "    carpeta_path = os.path.join(base_dir, carpeta)\n",
    "    path_expert = os.path.join(carpeta_path, 'terminos_validados_todos.txt')\n",
    "    path_model = os.path.join(carpeta_path, 'terminos_extraidos_yake.txt')\n",
    "\n",
    "    if not os.path.exists(path_expert) or not os.path.exists(path_model):\n",
    "        print(f\"⚠️ Archivos faltantes en {carpeta}\")\n",
    "        continue\n",
    "\n",
    "    with open(path_expert, 'r', encoding='utf-8') as f:\n",
    "        expert_terms = [line.strip().lower() for line in f if line.strip()]\n",
    "\n",
    "    with open(path_model, 'r', encoding='utf-8') as f:\n",
    "        candidate_terms = [line.strip().lower() for line in f if line.strip()]\n",
    "\n",
    "    r = evaluate_annotations(candidate_terms, expert_terms)\n",
    "    TP_total = r['TP_exact'] + r['TP_partial']\n",
    "    precision = TP_total / (TP_total + r['FP']) if TP_total + r['FP'] > 0 else 0.0\n",
    "    recall = TP_total / (TP_total + r['FN']) if TP_total + r['FN'] > 0 else 0.0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0.0\n",
    "    f2 = 5 * precision * recall / (4 * precision + recall) if precision + recall > 0 else 0.0\n",
    "    avg_lev = sum(r['partial_distances']) / len(r['partial_distances']) if r['partial_distances'] else None\n",
    "\n",
    "    resultados.append({\n",
    "        'Artículo': carpeta,\n",
    "        'TP_exact': r['TP_exact'],\n",
    "        'TP_partial': r['TP_partial'],\n",
    "        'FP': r['FP'],\n",
    "        'FN': r['FN'],\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1': f1,\n",
    "        'F2': f2,\n",
    "        'Avg_Norm_Levenshtein': avg_lev\n",
    "    })\n",
    "\n",
    "    for k in ['TP_exact', 'TP_partial', 'FP', 'FN']:\n",
    "        global_counts[k] += r[k]\n",
    "    global_counts['partial_distances'].extend(r['partial_distances'])\n",
    "\n",
    "# Resultados globales\n",
    "TP_total = global_counts['TP_exact'] + global_counts['TP_partial']\n",
    "FP = global_counts['FP']\n",
    "FN = global_counts['FN']\n",
    "precision = TP_total / (TP_total + FP) if TP_total + FP > 0 else 0.0\n",
    "recall = TP_total / (TP_total + FN) if TP_total + FN > 0 else 0.0\n",
    "f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0.0\n",
    "f2 = 5 * precision * recall / (4 * precision + recall) if precision + recall > 0 else 0.0\n",
    "avg_lev = sum(global_counts['partial_distances']) / len(global_counts['partial_distances']) if global_counts['partial_distances'] else None\n",
    "\n",
    "# Exportar resultados\n",
    "ruta_resultados_por_articulo = os.path.join(base_dir, 'evaluacion_anotaciones_por_articulo_yake.csv')\n",
    "ruta_resultados_global = os.path.join(base_dir, 'evaluacion_anotaciones_global_yake.csv')\n",
    "\n",
    "try:\n",
    "    df_resultados = pd.DataFrame(resultados)\n",
    "    df_resultados.to_csv(ruta_resultados_por_articulo, index=False, encoding='utf-8')\n",
    "    print(f\"📄 Resultados por artículo guardados en: {ruta_resultados_por_articulo}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error al guardar resultados por artículo: {e}\")\n",
    "\n",
    "try:\n",
    "    df_global = pd.DataFrame([{\n",
    "        'TP_exact': global_counts['TP_exact'],\n",
    "        'TP_partial': global_counts['TP_partial'],\n",
    "        'FP': FP,\n",
    "        'FN': FN,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1': f1,\n",
    "        'F2': f2,\n",
    "        'Avg_Norm_Levenshtein': avg_lev\n",
    "    }])\n",
    "    df_global.to_csv(ruta_resultados_global, index=False, encoding='utf-8')\n",
    "    print(f\"📄 Resultados globales guardados en: {ruta_resultados_global}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error al guardar resultados globales: {e}\")\n",
    "\n",
    "print(\"\\n✅ Evaluación completada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63744c25-1b68-4a09-ab1e-776ebed6aee0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
